{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRqKWtmttMQ5"
   },
   "source": [
    "# DataLoader\n",
    "In the previous notebook you have implemented a dataset that we can now use to access our data. However, in machine learning, we often need to perform a few additional data preparation steps before we can start training models.\n",
    "\n",
    "An important additional class for data preparation is the **DataLoader**. By wrapping a dataset in a dataloader, we will be able to load small subsets of the dataset at a time, instead of having to load each sample separately. In machine learning, the small subsets are referred to as **mini-batches**, which will play an important role later in the lecture.\n",
    "\n",
    "In this notebook, you will implement your own dataloader, which you can then use to load mini-batches from the dataset you implemented previously.\n",
    "\n",
    "First, you need to import libraries and code, as always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FdzZnueotMQ-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from exercise_code.data import DataLoader, DummyDataset\n",
    "from exercise_code.tests import (\n",
    "    test_dataloader, \n",
    "    test_dataloader_len,\n",
    "    test_dataloader_iter,\n",
    "    save_pickle, \n",
    "    load_pickle\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DHupqpWtMRA"
   },
   "source": [
    "## 1. Iterating over a Dataset\n",
    "Throughout this notebook a dummy dataset will be used that contains all even numbers from 2 to 100. Similar to the dataset you have implemented before, the dummy dataset has a `__len__()` method that allows us to call `len(dataset)`, as well as a `__getitem__()` method, which allows you to call `dataset[i]` and returns a dict `{\"data\": val}` where `val` is the i-th even number. If you would like to see the code, have a look at `DummyDataset` in `exercise_code/data/base_dataset.py`.\n",
    "\n",
    "Let's start by defining the dataset, and calling its methods to get a better feel for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NncZM01utMRA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Length:\t 50 \n",
      "First Element:\t {'data': 2} \n",
      "Last Element:\t {'data': 100}\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.data.base_dataset import DummyDataset\n",
    "\n",
    "dataset = DummyDataset(\n",
    "    root=None,\n",
    "    divisor=2,\n",
    "    limit=100\n",
    ")\n",
    "print(\n",
    "    \"Dataset Length:\\t\", len(dataset),\n",
    "    \"\\nFirst Element:\\t\", dataset[0],\n",
    "    \"\\nLast Element:\\t\", dataset[-1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMvaTMCHtMRB"
   },
   "source": [
    "In the following, you will write some code to iterate over the dataset in mini-batches, similarly to what a dataloader is supposed to do. The number of samples to load per mini-batch is called **batch size**. For the remainder of this notebook, the batch size is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Qgh_mlh8tMRB"
   },
   "outputs": [],
   "source": [
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCa09KiNtMRC"
   },
   "source": [
    "Let us now define a simple function that iterates over the dataset and groups samples into mini-batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fOBf6eb2tMRC"
   },
   "outputs": [],
   "source": [
    "def build_batches(dataset, batch_size):\n",
    "    batches = []  # list of all mini-batches\n",
    "    batch = []  # current mini-batch\n",
    "    for i in range(len(dataset)):\n",
    "        batch.append(dataset[i])\n",
    "        if len(batch) == batch_size:  # if the current mini-batch is full,\n",
    "            batches.append(batch)  # add it to the list of mini-batches,\n",
    "            batch = []  # and start a new mini-batch\n",
    "    return batches\n",
    "\n",
    "batches = build_batches(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFpC6_DetMRD"
   },
   "source": [
    "Let's have a look at the mini-batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yzQr2sSAtMRE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini-batch 0: [{'data': 2}, {'data': 4}, {'data': 6}]\n",
      "mini-batch 1: [{'data': 8}, {'data': 10}, {'data': 12}]\n",
      "mini-batch 2: [{'data': 14}, {'data': 16}, {'data': 18}]\n",
      "mini-batch 3: [{'data': 20}, {'data': 22}, {'data': 24}]\n",
      "mini-batch 4: [{'data': 26}, {'data': 28}, {'data': 30}]\n",
      "mini-batch 5: [{'data': 32}, {'data': 34}, {'data': 36}]\n",
      "mini-batch 6: [{'data': 38}, {'data': 40}, {'data': 42}]\n",
      "mini-batch 7: [{'data': 44}, {'data': 46}, {'data': 48}]\n",
      "mini-batch 8: [{'data': 50}, {'data': 52}, {'data': 54}]\n",
      "mini-batch 9: [{'data': 56}, {'data': 58}, {'data': 60}]\n",
      "mini-batch 10: [{'data': 62}, {'data': 64}, {'data': 66}]\n",
      "mini-batch 11: [{'data': 68}, {'data': 70}, {'data': 72}]\n",
      "mini-batch 12: [{'data': 74}, {'data': 76}, {'data': 78}]\n",
      "mini-batch 13: [{'data': 80}, {'data': 82}, {'data': 84}]\n",
      "mini-batch 14: [{'data': 86}, {'data': 88}, {'data': 90}]\n",
      "mini-batch 15: [{'data': 92}, {'data': 94}, {'data': 96}]\n"
     ]
    }
   ],
   "source": [
    "def print_batches(batches):  \n",
    "    for i, batch in enumerate(batches):\n",
    "        print(\"mini-batch %d:\" % i, str(batch))\n",
    "\n",
    "print_batches(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1QKM6UTtMRE"
   },
   "source": [
    "As you can see, the iteration works, but the output is not very pretty. Let us now write a simple function that combines the dictionaries of all samples in a mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "J9EoaMButMRF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini-batch 0: {'data': [2, 4, 6]}\n",
      "mini-batch 1: {'data': [8, 10, 12]}\n",
      "mini-batch 2: {'data': [14, 16, 18]}\n",
      "mini-batch 3: {'data': [20, 22, 24]}\n",
      "mini-batch 4: {'data': [26, 28, 30]}\n",
      "mini-batch 5: {'data': [32, 34, 36]}\n",
      "mini-batch 6: {'data': [38, 40, 42]}\n",
      "mini-batch 7: {'data': [44, 46, 48]}\n",
      "mini-batch 8: {'data': [50, 52, 54]}\n",
      "mini-batch 9: {'data': [56, 58, 60]}\n",
      "mini-batch 10: {'data': [62, 64, 66]}\n",
      "mini-batch 11: {'data': [68, 70, 72]}\n",
      "mini-batch 12: {'data': [74, 76, 78]}\n",
      "mini-batch 13: {'data': [80, 82, 84]}\n",
      "mini-batch 14: {'data': [86, 88, 90]}\n",
      "mini-batch 15: {'data': [92, 94, 96]}\n"
     ]
    }
   ],
   "source": [
    "def combine_batch_dicts(batch):\n",
    "    batch_dict = {}\n",
    "    for data_dict in batch:\n",
    "        for key, value in data_dict.items():\n",
    "            if key not in batch_dict:\n",
    "                batch_dict[key] = []\n",
    "            batch_dict[key].append(value)\n",
    "    return batch_dict\n",
    "\n",
    "combined_batches = [combine_batch_dicts(batch) for batch in batches]\n",
    "print_batches(combined_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRfhnqxjtMRF"
   },
   "source": [
    "This looks much more organized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mZ2pkN-tMRG"
   },
   "source": [
    "To perform operations more efficiently later, we would also like the values of the mini-batches to be contained in a numpy array instead of a simple list. Let's briefly write a function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "u_mCr-HVtMRG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini-batch 0: {'data': array([2, 4, 6])}\n",
      "mini-batch 1: {'data': array([ 8, 10, 12])}\n",
      "mini-batch 2: {'data': array([14, 16, 18])}\n",
      "mini-batch 3: {'data': array([20, 22, 24])}\n",
      "mini-batch 4: {'data': array([26, 28, 30])}\n",
      "mini-batch 5: {'data': array([32, 34, 36])}\n",
      "mini-batch 6: {'data': array([38, 40, 42])}\n",
      "mini-batch 7: {'data': array([44, 46, 48])}\n",
      "mini-batch 8: {'data': array([50, 52, 54])}\n",
      "mini-batch 9: {'data': array([56, 58, 60])}\n",
      "mini-batch 10: {'data': array([62, 64, 66])}\n",
      "mini-batch 11: {'data': array([68, 70, 72])}\n",
      "mini-batch 12: {'data': array([74, 76, 78])}\n",
      "mini-batch 13: {'data': array([80, 82, 84])}\n",
      "mini-batch 14: {'data': array([86, 88, 90])}\n",
      "mini-batch 15: {'data': array([92, 94, 96])}\n"
     ]
    }
   ],
   "source": [
    "def batch_to_numpy(batch):\n",
    "    numpy_batch = {}\n",
    "    for key, value in batch.items():\n",
    "        numpy_batch[key] = np.array(value)\n",
    "    return numpy_batch\n",
    "\n",
    "numpy_batches = [batch_to_numpy(batch) for batch in combined_batches]\n",
    "print_batches(numpy_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pr6V9V-HtMRG"
   },
   "source": [
    "Lastly, we would like to make the loading a bit more memory efficient. Instead of loading the entire dataset into memory at once, let us only load samples when they are needed. This can also be done by building a Python generator, using the `yield` keyword. See https://wiki.python.org/moin/Generators for more information on generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "K2Z49QWmtMRG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini-batch 0: {'data': array([ 74,  44, 100])}\n",
      "mini-batch 1: {'data': array([86, 34, 90])}\n",
      "mini-batch 2: {'data': array([62, 28,  6])}\n",
      "mini-batch 3: {'data': array([56, 68, 60])}\n",
      "mini-batch 4: {'data': array([12, 72, 18])}\n",
      "mini-batch 5: {'data': array([ 8, 46, 96])}\n",
      "mini-batch 6: {'data': array([78, 88, 94])}\n",
      "mini-batch 7: {'data': array([14, 10, 70])}\n",
      "mini-batch 8: {'data': array([38, 98, 84])}\n",
      "mini-batch 9: {'data': array([40, 58, 92])}\n",
      "mini-batch 10: {'data': array([20, 16, 66])}\n",
      "mini-batch 11: {'data': array([ 2,  4, 32])}\n",
      "mini-batch 12: {'data': array([36, 42, 64])}\n",
      "mini-batch 13: {'data': array([30, 24, 50])}\n",
      "mini-batch 14: {'data': array([52, 48, 82])}\n",
      "mini-batch 15: {'data': array([22, 76, 26])}\n"
     ]
    }
   ],
   "source": [
    "def build_batch_iterator(dataset, batch_size, shuffle):\n",
    "    if shuffle:\n",
    "        index_iterator = iter(np.random.permutation(len(dataset)))  # define indices as iterator\n",
    "    else:\n",
    "        index_iterator = iter(range(len(dataset)))  # define indices as iterator\n",
    "\n",
    "    batch = []\n",
    "    for index in index_iterator:  # iterate over indices using the iterator\n",
    "        batch.append(dataset[index])\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch  # use yield keyword to define a iterable generator\n",
    "            batch = []\n",
    "            \n",
    "batch_iterator = build_batch_iterator(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "batches = []\n",
    "for batch in batch_iterator:\n",
    "    batches.append(batch)\n",
    "\n",
    "print_batches(\n",
    "    [batch_to_numpy(combine_batch_dicts(batch)) for batch in batches]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xuBppBltMRH"
   },
   "source": [
    "The functionality of the cell above is now pretty close to what the dataloader is supposed to do. However, there are still two remaining issues:\n",
    "1. The last two samples of the dataset are not contained in any mini-batch. This is because the number of samples in the dataset is not dividable by the batch size, so there are a few left-over samples which are implicitly discarded. Ideally, an option would be prefered that allows you to decide how to handle these last samples.\n",
    "2. The order of the mini-batches, as well as the fact which samples are grouped together, is always in increasing order. Ideally, there should be another option that allows you to randomize which samples are grouped together. The randomization could be easily implemented by randomly permuting the indices of the dataset before iterating over it, e.g. using `indices = np.random.permutation(len(dataset))`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSQFZkZ8tMRH"
   },
   "source": [
    "## 2. DataLoader Class Implementation\n",
    "Now it is your turn to put everything together and implement the DataLoader as a proper class.\n",
    "We provide you with a basic skeleton for this, which you can find in `class DataLoader` of `exercise_code/data/image_folder_dataset.py`. Open the file and have a look at the class. Note that the `__init__` method receives four arguments:\n",
    "* **dataset** is the dataset that the dataloader should load.\n",
    "* **batch_size** is the mini-batch size, i.e. the number of samples you want to load at a time.\n",
    "* **shuffle** is binary and defines whether the dataset should be randomly shuffled or not.\n",
    "* **drop_last**: is binary and defines how to handle the last mini-batch in your dataset. Specifically, if the amount of samples in your dataset is not dividable by the mini-batch size, there will be some samples left over in the end. If `drop_last=True`, we simply discard those samples, otherwise we return them together as a smaller mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXxpeH6ntMRH"
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Implement the <code>__len__(self)</code> method in <code>exercise_code/data/dataloader.py</code>. </p>\n",
    "    <p><b>Hint:</b> Don't forget to think about drop_last! We will test for both modes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "MK305ceZtMRI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LenTestInt passed.\n",
      "LenTestCorrect passed.\n",
      "Method __len__() using drop_last=True correctly implemented. Tests passed: 2/2\n",
      "\n",
      "LenTestInt passed.\n",
      "LenTestCorrect passed.\n",
      "Method __len__() using drop_last=False correctly implemented. Tests passed: 2/2\n",
      "\n",
      "Method __len__() correctly implemented. Tests passed: 4/4\n",
      "Score: 100/100\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.data.dataloader import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "_ = test_dataloader_len(\n",
    "    dataloader=dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZNXgKCrtMRI"
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Implement the <code>__iter__(self)</code> method in <code>exercise_code/data/dataloader.py</code>. </p>\n",
    "    <p><b>Hint:</b> Make use of the code in '1. Iterating over a Dataset' when implementing your <code>__iter__()</code> method. We are again testing for both drop_last modes! \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "G21NSLYMtMRI",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterTestIterable passed.\n",
      "IterTestItemType passed.\n",
      "IterTestBatchSize passed.\n",
      "IterTestNumBatches passed.\n",
      "IterTestValuesUnique passed.\n",
      "IterTestValueRange passed.\n",
      "IterTestShuffled passed.\n",
      "IterTestNonDeterministic passed.\n",
      "Method __iter__() using drop_last=True correctly implemented. Tests passed: 8/8\n",
      "\n",
      "IterTestIterable passed.\n",
      "IterTestItemType passed.\n",
      "IterTestBatchSize passed.\n",
      "IterTestNumBatches passed.\n",
      "IterTestValuesUnique passed.\n",
      "IterTestValueRange passed.\n",
      "IterTestShuffled passed.\n",
      "IterTestNonDeterministic passed.\n",
      "Method __iter__() using drop_last=False correctly implemented. Tests passed: 8/8\n",
      "\n",
      "Method __iter__() correctly implemented. Tests passed: 16/16\n",
      "Score: 100/100\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.data.dataloader import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "_ = test_dataloader_iter(\n",
    "    dataloader=dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwbhUpr_tMRI"
   },
   "source": [
    "If you're done, run the cells below to check if your dataloader works as intended. You can change the value of drop_last to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "T7wuWEs0tMRJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([42, 88, 64])}\n",
      "{'data': array([84, 12, 68])}\n",
      "{'data': array([18, 62, 38])}\n",
      "{'data': array([20,  2, 96])}\n",
      "{'data': array([16, 34, 14])}\n",
      "{'data': array([90, 80, 36])}\n",
      "{'data': array([70, 44, 78])}\n",
      "{'data': array([24, 74, 86])}\n",
      "{'data': array([10, 52,  4])}\n",
      "{'data': array([92, 66,  6])}\n",
      "{'data': array([30, 26, 58])}\n",
      "{'data': array([60, 22, 46])}\n",
      "{'data': array([100,  76,  94])}\n",
      "{'data': array([98, 40, 48])}\n",
      "{'data': array([ 8, 54, 82])}\n",
      "{'data': array([32, 28, 72])}\n",
      "{'data': array([56, 50])}\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.data.dataloader import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,    # Change here if you want to see the impact of drop last and check out the last batch\n",
    ")\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwXZb3GItMRJ"
   },
   "source": [
    "### Save your DataLoaders for Submission\n",
    "Simply save your dataloaders using the following cell. This will save them as well as dataset from the first notebook to a pickle file `cifar_dataset_and_loader.p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DwQet5dtMRJ"
   },
   "outputs": [],
   "source": [
    "from exercise_code.data.dataloader import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "dataset = load_pickle(\"cifar_dataset.p\") # load dataset from the pickle file saved in notbook 1\n",
    "\n",
    "save_pickle(\n",
    "    data_dict={\n",
    "        \"dataset\": dataset['dataset'],\n",
    "        \"cifar_mean\": dataset['cifar_mean'],\n",
    "        \"cifar_std\": dataset['cifar_std'],\n",
    "        \"dataloader\": dataloader\n",
    "    },\n",
    "    file_name=\"cifar_dataset_and_loader.p\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fm9enwNZtMRL"
   },
   "outputs": [],
   "source": [
    "from exercise_code.submit import submit_exercise\n",
    "\n",
    "submit_exercise('exercise03')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blXONbBbtMRL"
   },
   "source": [
    "# Key Takeaways\n",
    "1. In machine learning, we often need to load data in **mini-batches**, which are small subsets of the training dataset. How many samples to load per mini-batch is called the **batch size**.\n",
    "2. In addition to the Dataset class, we use a **DataLoader** class that takes care of mini-batch construction, data shuffling, and more.\n",
    "3. The dataloader is iterable and only loads those samples of the dataset that are needed for the current mini-batch. This can lead to bottlenecks later if you are unable to provide enough batches in time for your upcoming pipeline. This is especially true when loading from HDDs as the slow reading time can be a bottleneck in your complete pipeline later.\n",
    "4. The dataloader task can easily by distributed amongst multiple processes as well as pre-fetched. When we switch to PyTorch later we can directly use our dataset classes and replace our current Dataloader with theirs :)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8b_FX3utMRM"
   },
   "source": [
    "# Outlook\n",
    "You have now implemented everything you need to use the CIFAR datasets for deep learning model training. Using your dataset and dataloader, your model training will later look something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bF5vDy1ptMRM"
   },
   "outputs": [],
   "source": [
    "dataset = DummyDataset(\n",
    "    root=None,\n",
    "    divisor=2,\n",
    "    limit=200,\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=3,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "model = lambda x: x\n",
    "for minibatch in dataloader:\n",
    "    model_output = model(minibatch)\n",
    "    # do more stuff... (soon)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2_dataloader.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
